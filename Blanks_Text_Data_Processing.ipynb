{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bobghaddar99/deep-learning/blob/main/Blanks_Text_Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Data Processing\n",
        "© 2023, Zaka AI, Inc. All Rights Reserved.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h6WchVcNOeFT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZcQeG5JrQh3"
      },
      "source": [
        "# Text Cleaning\n",
        "\n",
        "This collab will show you how to clean your data in preparation for the NLP.\n",
        "\n",
        "In the first section, we will be using built-in python functions; as for the second, we will introduce the NLTK libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using built-in Python Functions"
      ],
      "metadata": {
        "id": "OzI3VWIzbigH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will perform the following cleaning steps:\n",
        "\n",
        "\n",
        "*   Split by white space\n",
        "*   Split by words\n",
        "\n",
        "*  Normalizing case\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cXjWXmzUb7Zf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4C6w-3Hy2LJ"
      },
      "source": [
        "###Split by white space\n",
        "\n",
        "The first step in text cleaning is splitting a document or text by word.\n",
        "Applying split() with no input parameters calls the function to split text by looking at **white spaces only**.\n",
        "\n",
        "\"Who's\" for example is considered an entire word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5nBPnVfUQms"
      },
      "source": [
        "text = 'Albert Einstein is widely celebrated as one of the most brilliant scientists who’s ever lived. His fame was due to his original and creative theories that at first seemed crazy, but that later turned out to represent the actual physical world.  Nonetheless, when he applied his theory of general relativity to the universe as a whole in a paper published in 1917, while serving as Director of the Kaiser Wilhelm Institute for Physics and professor at the University of Berlin, Einstein suggested the notion of a \"cosmological constant\". He discarded this notion when it had been established that the universe was indeed expanding. His contributions to physics made it possible to envision how the universe evolved.'\\\n",
        "      'In order to understand Einstein’s contribution to cosmology it is helpful to begin with his theory of gravity. Rather than thinking of gravity as an attractive force between two objects, in the tradition of Isaac Newton, Einstein’s conception was that gravity is a property of massive objects that “bends” space and time around itself. For example, consider the question of why the Moon does not fly off into space, rather than staying in orbit around Earth. Newton would say that gravity is a force acting between the Earth and Moon, holding it in orbit. Einstein would say that the massive Earth “bends” space and time around itself, so that the moon follows the curves created by the massive Earth. His theory was confirmed when he predicted that even starlight would bend when passing near the sun during a solar eclipse.'\\\n",
        "      'In 1917 Einstein published a paper in which he applied this theory to all matter in space. His theory led to the conclusion that all the mass in the universe would bend space so much that it should have long ago contracted into a single dense blob. Given that the universe seems pretty well spread out, however, and does not seem to be contracting, Einstein decided to add a “fudge factor,” that acts like “anti-gravity” and prevents the universe from collapsing. He called this idea, which was represented as an additional term in the mathematical equation representing his theory of gravity, the cosmological constant. In other words, Einstein supposed the universe to be static and unchanging, because that is the way it looked to astronomers in 1917.'\n",
        "\n",
        "# split into words by white space\n",
        "words = ______\n",
        "print(words[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1nCWZyjzD81"
      },
      "source": [
        "### Split by word\n",
        "\n",
        "Another approach to split words is using regular expression **re**.\n",
        "\n",
        "In this case splitting is based on words and you can see the difference in \"who's\" which is not split into 'who' and 's'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nja7dzcyDNF"
      },
      "source": [
        "import re\n",
        "\n",
        "# split based on words only\n",
        "_________"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGy82Z14zwzT"
      },
      "source": [
        "### Normalizing case\n",
        "\n",
        "Normalizing is when we turn all the words of the document into lower case. Careful however not always employing this method because it might change the entire meaning.\n",
        "\n",
        "For example take the French telecom company Orange and the fruit orange. Normalizing would change the entire meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX-qZxxBzMKT"
      },
      "source": [
        "# split into words by white space\n",
        "_______\n",
        "\n",
        "# convert to lower case\n",
        "_____________"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LVpZgMF0PAc"
      },
      "source": [
        "## Using the NLTK libraries\n",
        "\n",
        "The Natural Language Toolkit is a suite of libraries and programs for symbolic and statistical natural language processing for English, written in the Python programming language. *(Wikipedia)*\n",
        "\n",
        "We will apply the following steps:\n",
        "\n",
        "\n",
        "*   Split into sentences\n",
        "*   Split into words\n",
        "*   Filter out punctuation\n",
        "*   Remove stopwords\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIbzFZArGKKB"
      },
      "source": [
        "### Split into sentences\n",
        "\n",
        "The ***sent_tokenize()*** tokenizer divides a text into a list of sentences using an unsupervised learning algorithm to learn what constitutes a sentence break. It is unsupervised because you don't have to give it any labeled training data, just raw text. It must be trained on a large collection of plaintext in the target language before it can be used.\n",
        "\n",
        "The NLTK data package includes a pre-trained Punkt tokenizer for English which we will import. *(nltk.org)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gud_Lu2o0Uv3"
      },
      "source": [
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# split into sentences\n",
        "_________\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqjS6Vpp1NFE"
      },
      "source": [
        "### Split into words\n",
        "\n",
        "From the same toolkit, we consider the tokenize library and import the word tokenizer. Similary to `re.split`, this function will split the text into tokens rather than words.\n",
        "\n",
        "*Make sure you check out the ouput and spot the differences!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH243FyW079Y"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# split into words\n",
        "__________"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHBg2ySw1nvd"
      },
      "source": [
        "### Filter out punctuation\n",
        "\n",
        "Python includes the built-in function `isalpha()` that can be used in order to determine whether or not the scanned word is alphabetical or else (numerical, punctuation, special characters, etc...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO_XJoqQ1T7f"
      },
      "source": [
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# remove all tokens that are not alphabetic\n",
        "______________"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymzbbr-l2i4F"
      },
      "source": [
        "### Remove stopwords\n",
        "\n",
        "Stopwords are the words which do not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. The most common are short function words such as the, is, at, which, and on, etc.\n",
        "\n",
        "In this case, removing stopwords can cause problems when searching for phrases that include them, particularly in names such as “The Who” or “Take That”.\n",
        "\n",
        "Including the word \"not\" as a stopword also changes the entire meaning if removed (try \"this code is not good\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83lGVicu2cuQ"
      },
      "source": [
        "# let's list all the stopwords for NLTK\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeRZd4vT3Wn8"
      },
      "source": [
        "As you can see, the stopwords are all lower case, so if we're to compare them with our tokens, we need to make sure that our text is prepared the same way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHmDBFUS7zQk"
      },
      "source": [
        "This cell recaps all what we have previously learnt in this colab: tokenizing, lower casing and checking for alphabetic words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUheZUYD2prw"
      },
      "source": [
        "# clean our text\n",
        "\n",
        "# split into words\n",
        "___________\n",
        "\n",
        "# convert to lower case\n",
        "___________\n",
        "\n",
        "# remove all tokens that are not alphabetic\n",
        "___________\n",
        "\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "___________\n",
        "print(words[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI8aqq274DZS"
      },
      "source": [
        "### Stem words\n",
        "\n",
        "Stemming refers to the process of reducing each word to its root or base.\n",
        "For the English language, you can choose between PorterStemmer or LancasterStemmer for suffix stripping, each has its own algorithm and sometimes display different outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7PZmd_636vW"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# stemming of words\n",
        "porter = PorterStemmer()\n",
        "___________\n",
        "print(stemmed[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTZRgaRQ4ig-"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "\n",
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# stemming of words\n",
        "lancaster = LancasterStemmer()\n",
        "___________\n",
        "print(stemmed[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "eOcBMgy4NeGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QUkAvYPjRrMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create WordNetLemmatizer object\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "___________\n",
        "\n",
        "#print the stemmed vs the lemmatized tokens\n",
        "___________\n"
      ],
      "metadata": {
        "id": "hc0ZCQTHVJYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, unlike stemming, the lemmatizer mapped words to its root forms while ensuring they belong to the language."
      ],
      "metadata": {
        "id": "f41S2Tkpksgp"
      }
    }
  ]
}