{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bobghaddar99/deep-learning/blob/main/Blanks_TS_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCZJ7MpmYBgW"
      },
      "source": [
        "# Time-series Prediction\n",
        "---\n",
        "\n",
        "Â© 2023, Zaka AI, Inc. All Rights Reserved\n",
        "\n",
        "## Case Study: Energy Consumption Prediction in Households\n",
        "\n",
        "**Objective:**\n",
        "\n",
        "In this exercise, you will learn proper practices of working with time-series data for prediction tasks.\n",
        "\n",
        "With this, you would have completed all fundamental concepts of building prediction models for time-series data.  \n",
        "\n",
        "---\n",
        "## Table of Contents\n",
        "\n",
        "### Section 1. Exploratory Data Analysis\n",
        "\n",
        "* 1.1: Data Loading and Understanding\n",
        "* 1.2: Data Cleaning\n",
        "* 1.3: Data Visualization\n",
        "\n",
        "### Section 2. Time-series Data Preparation for Forecasting\n",
        "\n",
        "### Section 3. Building Time-series Models\n",
        "\n",
        "### Section 4. Evaluating Time-series Models\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiMK_J0cY_hS"
      },
      "source": [
        "## 0. Scenario, Problem & Dataset Description\n",
        "\n",
        "Today, you will dive into a dataset collected from a home in Paris, which we would like to use to **learn** and **make predictions** of the household owner's energy consumption patterns. The dataset collects 2,075,259 measurements between December 2006 and November 2010 (so we have 47 months of data to work with).\n",
        "\n",
        "### The data is collected from multiple smart meters and contains the following attributes/features:\n",
        "\n",
        "- date: Date in format dd/mm/yyyy\n",
        "\n",
        "- time: time in format hh:mm:ss\n",
        "\n",
        "- global_active_power: household global minute-averaged active power (in kilowatt)\n",
        "\n",
        "- global_reactive_power: household global minute-averaged reactive power (in kilowatt)\n",
        "\n",
        "- voltage: minute-averaged voltage (in volt)\n",
        "\n",
        "- global_intensity: household global minute-averaged current intensity (in ampere)\n",
        "\n",
        "- sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n",
        "\n",
        "- sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n",
        "\n",
        "- sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\n",
        "\n",
        "### Some notes we have about the dataset are:\n",
        "1.  (global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.\n",
        "\n",
        "2.   The dataset contains some missing values in the measurements (nearly 1.25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows *missing values on April 28, 2007*.\n",
        "\n",
        "You can find the dataset description [here](https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X15s-QlQZUKi"
      },
      "source": [
        "# **1. Exploratory Data Analysis**\n",
        "\n",
        "### Importing Libraries\n",
        "\n",
        "First off, let's import the python libraries we know we'll typically be using.\n",
        "\n",
        "Some of the common ones are:\n",
        "\n",
        "*   [numpy](https://www.numpy.org/) - supports large, multidimensional arrays and has a lot of useful mathematical built-in functions to run on these arrays\n",
        "*   [pandas](https://pandas.pydata.org/) - offers high-performance, easy-to-use data structures (e.g. can store data of multiple types in one data structure)\n",
        "*   [matplotlib](https://matplotlib.org/) - 2D plotting library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe8XpLh2bd4a"
      },
      "source": [
        "---\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-Yo3xW_bd_1"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "You can download the dataset [here](https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption).\n",
        "\n",
        "We will be cloning and downloading the dataset from Github."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM0Q8aZMXhvQ"
      },
      "source": [
        "!git clone https://github.com/zaka-ai/time-series-course.git\n",
        "\n",
        "%cd time-series-course"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dty4jqBzUAAw"
      },
      "source": [
        "The data is in a ZIP file so we need to unzip first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_9pnjHHaf3R"
      },
      "source": [
        "!unzip household_power_consumption.txt.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNIj4D7zahmw"
      },
      "source": [
        "path_to_data = 'household_power_consumption.txt'\n",
        "data = ---\n",
        "\n",
        "# quickly visualize - print first 5 rows\n",
        "---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3BKR81Is7E0"
      },
      "source": [
        "#### Some points to pay attention to:\n",
        "\n",
        "*   Our data is collected every minute.\n",
        "*   We have measurements collected in different units.\n",
        "*   What are our descriptors? What is our target?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYmSK9WctlGj"
      },
      "source": [
        "To answer the last question above, we need to define our problem. What are we trying to do?\n",
        "\n",
        "We can choose to frame the problem as one of the following:\n",
        "\n",
        "1.  We can predict a household's total power consumption. This can be useful for:\n",
        "\n",
        "> *   Demand planning by power distribution companies\n",
        "> *   Powering/connecting to renewable energy sources\n",
        "> *   Financial planning of users\n",
        "\n",
        "2.  We can predict the household's consumption of specific power devices. This can be useful for:\n",
        "\n",
        "> *   Users can better plan their consumption of loads\n",
        "> *   Smart grids can switch off low-demand devices during peak demand hours (e.g. fridge)\n",
        "\n",
        "Today, we will be answering a specific question:\n",
        "\n",
        "***Given recent power consumption, what is the expected total power consumption for the next week?***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyP0uqya6Qen"
      },
      "source": [
        "### Data Inspection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-ZPIPRSkAyY"
      },
      "source": [
        "# inspect types\n",
        "---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPdTKRZslAnI"
      },
      "source": [
        "# let's get a look at summary of statistics\n",
        "---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHAE_VJuw4rp"
      },
      "source": [
        "data.describe(include='all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StpMtOwfpV8d"
      },
      "source": [
        "# data shape\n",
        "rows, cols = ---\n",
        "print(f\"The dataset is composed of {rows} rows and {cols} columns.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEOgVDTW0OCv"
      },
      "source": [
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d700CVy70yXB"
      },
      "source": [
        "### Data Cleaning\n",
        "\n",
        "In the `describe()` summary, we could see a symbol \"?\" showing in our data. Perhaps this is what the data collectors used to resemble missing values. Let's check April 29 of 2007 that we already know has a missing value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY_xdkV7xQqJ"
      },
      "source": [
        "We can make use of the date and time columns to easily search through our data. We will turn the date and time to indicies to easily navigate!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov0tLuO_w9iS"
      },
      "source": [
        "data = pd.read_csv(\"household_power_consumption.txt\", sep=';', infer_datetime_format=True, parse_dates={'datetime':[0,1]}, index_col=['datetime'])\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXKjZt_R2yXt"
      },
      "source": [
        "Now we can easily search our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdXjStK_0OJc"
      },
      "source": [
        "import datetime\n",
        "# search for April 29, 2007 using the datetime index on the dataframe\n",
        "data.loc[datetime.datetime(year=2007,month=4,day=29)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAAL_EhN2q47"
      },
      "source": [
        "**Finding:** \"?\" is a symbol used to represent missing values!\n",
        "\n",
        "Let's take care of these missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wisnj1-r3Kd0"
      },
      "source": [
        "# Removing rows with missing values or \"?\"\n",
        "\n",
        "# First. let's convert \"?\"\" to nan\n",
        "---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX17B3FO9CEd"
      },
      "source": [
        "# Let's check April 29, 2007 to see if we have properly replaced \"?\" with NaN\n",
        "data.loc[datetime.datetime(year=2007,month=4,day=29)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc-sr5qhBXoK"
      },
      "source": [
        "# So our data should be all float types, let's check\n",
        "---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPHpy8i6BcQ1"
      },
      "source": [
        "# We need to change the data from string to float\n",
        "data = data.astype('float32')\n",
        "data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "530D-k9t3N1-"
      },
      "source": [
        "# Second. let's check how many missing values we have\n",
        "---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JcfTnhX3PY7"
      },
      "source": [
        "# One solution we could use is to drop all rows with missing values\n",
        "data_remove = data.dropna()\n",
        "data_remove.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orkfCueY4Mil"
      },
      "source": [
        "2,075,259 - 2,049,280 = we dropped 25979 rows!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do7Zc5b_3UZW"
      },
      "source": [
        "# Let's check April 29, 2007 to see if we have properly removed missing values from the dataset\n",
        "# data_remove.loc[datetime.date(year=2007,month=4,day=29)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd4Fn8ggBIUR"
      },
      "source": [
        "data_remove.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f67AikHlBzvE"
      },
      "source": [
        "# But we don't want to lose this many samples (25K!) so let's replace the missing values with the mean\n",
        "data = ---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaQf0lQtCL6D"
      },
      "source": [
        "# let's check if we have correctly replaced April 29, 2007 with the mean\n",
        "data.loc[datetime.datetime(year=2007,month=4,day=29)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B4_8PvsCTF3"
      },
      "source": [
        "# Any missing values left?\n",
        "---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpkalIO76AoW"
      },
      "source": [
        "**DONE** We have cleaned our data from missing values.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCgjVY5v6Yc1"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXGT4lGTxP3X"
      },
      "source": [
        "Each measurement/sample in a row is collected every minute of the day for 47 months! Do we need all this information?\n",
        "\n",
        "If we're doing a prediction for the next week level consumptions, then we don't need to know how much power is being consumed per minute (too specific). We could probably get by with the consumption level per hour. We can likely even get by per day.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KxnByI8yHp7"
      },
      "source": [
        "# we can easily resample minutes to days\n",
        "daily_data = ---\n",
        "\n",
        "# check new size of data (remember we lumped readings so data will shrink by an order of 1/(60 minutes per hour *24 hours per day))\n",
        "daily_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpgtn4J6_CoE"
      },
      "source": [
        "# let's take a look at how our index looks like now\n",
        "daily_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbcv2lUMCZlg"
      },
      "source": [
        "We now have our data sampled per day and in the proper setup for **one week ahead prediction.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I13zQwgnZXqv"
      },
      "source": [
        "### Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4w8IYCVZeHf"
      },
      "source": [
        "daily_data_meter_1 = daily_data['Sub_metering_1']\n",
        "daily_data_meter_2 = daily_data['Sub_metering_2']\n",
        "daily_data_meter_3 = daily_data['Sub_metering_3']\n",
        "\n",
        "daily_data_meter_1.plot(color=\"blue\") # can plot directly in pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A88Vk61Qc0qy"
      },
      "source": [
        "We can see how power consumption looks like for kitchen appliances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z7rCrIKcORG"
      },
      "source": [
        "# Can plot them all in one figure directly from pandas\n",
        "daily_data_per_meter = daily_data[['Sub_metering_1','Sub_metering_2','Sub_metering_3']]\n",
        "daily_data_per_meter.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38QFfmhGc5lu"
      },
      "source": [
        "It seems like `sub_metering_3` (which refers to heating and air conditioning) seems to have a unique pattern compared to the remaining measurements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PHW6CzZcQ-w"
      },
      "source": [
        "# Using matplotlib\n",
        "plt.figure()                # the first figure\n",
        "plt.figure(figsize=(10,5))  # define figure size (rows, cols)\n",
        "plt.suptitle('Usage of Kitchen, Laundry Room, and Heating Units Appliances')\n",
        "\n",
        "plt.subplot(311)             # the first subplot in the first figure\n",
        "plt.plot(daily_data['Sub_metering_1'],color=\"blue\")\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Watt-hour')\n",
        "\n",
        "plt.subplot(312)             # the second subplot in the first figure\n",
        "plt.plot(daily_data['Sub_metering_2'],color=\"orange\")\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Watt-hour')\n",
        "\n",
        "plt.subplot(313)             # the third subplot in the first figure\n",
        "plt.plot(daily_data['Sub_metering_3'],color=\"green\")\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Watt-hour')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nf6fYlfdDiI"
      },
      "source": [
        "**Question:** Do you notice anything about `sub_metering_3`?\n",
        "\n",
        "**Finding:** There is a clear seasonal component. This is due to high consumption of heating appliances during winter and low consumption during summers. This behavior would look different if we were collecting measurements from UAE, for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5FMuS0-ZeOM"
      },
      "source": [
        "# **2. Time-series Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t74nlcPdsNrG"
      },
      "source": [
        "OK, so now we should split our data into train and test sets. How do we do that? **Which columns are our descriptors and where's our target?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWY0hkvlZiup"
      },
      "source": [
        "# The first and last days in the data are:\n",
        "daily_data.iloc[[0]] # first day"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W_iXsW5nSKC"
      },
      "source": [
        "daily_data.iloc[[-1]] # last day"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXhD07z5nSUQ"
      },
      "source": [
        "Starting date: 2006-12-16\n",
        "Ending date: 2010-11-26\n",
        "\n",
        "If we want one week ahead prediction, it makes sense to use the previous week of data to predict the week ahead.\n",
        "\n",
        "To set this up properly, we would want our week to start on Monday and end on Sunday.\n",
        "\n",
        "> The **first Monday in the dataset** is December 18, 2006 (which is the third row in the dataset).\n",
        "\n",
        "> The **last Sunday in the dataset** is November 21, 2010. (which is the -6 from the end).\n",
        "\n",
        "Organizing the data into the step up above, we would have a total of 205 weeks.\n",
        "\n",
        "*NOPE.. didn't count them myself! Here's an online [calculator](https://planetcalc.com/274/?license=1).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTMFm4s4nuiV"
      },
      "source": [
        "daily_data.iloc[[2]] # First day for us"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONk9vyumnu-o"
      },
      "source": [
        "daily_data.iloc[[-5]] # Last day for us"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2a_H6ffnzfy"
      },
      "source": [
        "If we decide to divide our data into ~70% training and ~30% testing, then we would need:\n",
        "\n",
        "30% of 205 weeks = 61 weeks\n",
        "\n",
        "61 weeks = 427 days\n",
        "\n",
        "We are starting at day -5 so we need to read up to -(427+5)= -432. **Great!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGGqEXBC4l4l"
      },
      "source": [
        "\n",
        "> **Training & Testing samples are NOT randomly selected. Wait, what?!**\n",
        "\n",
        "Unlike other datasets, we will not randomly select our test and train samples. We will divide our dataset into the most recent readings being training samples and the later readings testing samples.\n",
        "\n",
        "This is because we are working on a prediction problem, and thus, the sequence in which data is presented matters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oigh1meoFaz"
      },
      "source": [
        "data = daily_data.values\n",
        "train, test = data[2:-432], data[-432:-5] # Important numbers (ignoring the lingering days that don't fit into our Monday-Sunday structure)\n",
        "\n",
        "# reshape into windows of weekly data (one week = 7 days) (total days / 7 = total weeks)\n",
        "train = ---\n",
        "test = ---\n",
        "\n",
        "# check shape\n",
        "print(f\"The training set is {train.shape} and the test set is {test.shape}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiU3EznVou6q"
      },
      "source": [
        "Where do these 3 dimensions come from?\n",
        "**(weeks, days, features)**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DkrZ_tZ04_M"
      },
      "source": [
        "Here, we would usually normalize our data columns since we don't want our model to get biased towards specific features.\n",
        "\n",
        "However, this is not NECESSARY when dealing with time-series prediction as our *features* are the data observations themselves.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX-eA8Yv3T1C"
      },
      "source": [
        "So, now we have the training and testing sets. One more thing we need to prepare before moving to modeling is:\n",
        "\n",
        "**How do we setup the data for supervised learning?** That is, what is my *X* and what is my *y*?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K1NwQd33rau"
      },
      "source": [
        "At each instant, we want to feed the model a week, and predict the week ahead. We don't do this prediction for Mondays only; we do them for every day of the week.\n",
        "\n",
        "Meaning, we want our input (X) and output (y) to look like:\n",
        "\n",
        "```\n",
        "[Input], [Output]\n",
        "[d01, d02, d03, d04, d05, d06, d07], [d08, d09, d10, d11, d12, d13, d14]\n",
        "[d02, d03, d04, d05, d06, d07, d08], [d09, d10, d11, d12, d13, d14, d15]\n",
        "Etc ...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rWpW5jK3lyy"
      },
      "source": [
        "# flatten the train data over all weeks\n",
        "train = --- # shape: [weeks*days, sensors]\n",
        "# check shape\n",
        "train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-c7x6mi6wkh"
      },
      "source": [
        "# flatten the test data over all weeks\n",
        "test = test.reshape((test.shape[0]*test.shape[1], test.shape[2])) # shape: [weeks*days, sensors]\n",
        "# check shape\n",
        "test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjG-FLHR4QZz"
      },
      "source": [
        "def supervised_setup(data, column):\n",
        "  # data: expects train/test set with 2 dimensions of (samples, features)- i.e. one sensor reading\n",
        "  # column: expects integer indicating column number of meter of interest\n",
        "\n",
        "  X, y = [], [] # start with empty lists for X and y\n",
        "  input_start = 0 # iterator\n",
        "  n_input = 7 # we want 7 days as input\n",
        "  n_out = 7 # we want 7 days as output\n",
        "\n",
        "  # step over the entire history one time step at a time\n",
        "  for i in range(len(data)):\n",
        "\t  # define the end of the input and corresponding output\n",
        "\t  input_end = input_start + n_input\n",
        "\t  output_end = input_end + n_out\n",
        "\n",
        "    # ensure we have enough data for this instance\n",
        "\t  if output_end < len(data):\n",
        "\t\t  x_input = data[input_start:input_end, column]\n",
        "\t\t  x_input = x_input.reshape((len(x_input), 1))\n",
        "\n",
        "\t\t  X.append(x_input)\n",
        "\t\t  y.append(data[input_end:output_end, column])\n",
        "\n",
        "\t  # move along one time step\n",
        "\t  input_start += 1\n",
        "  return np.array(X), np.array(y)\n",
        "\n",
        "# TRY IT:\n",
        "\n",
        "# split the data for the \"total active power\", i.e. column = 0\n",
        "X_train, y_train = supervised_setup(train, 0)\n",
        "X_test, y_test = supervised_setup(test, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBkQAVq-ZmC9"
      },
      "source": [
        "# **3. Building Time-series Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfPGRv_M8L6n"
      },
      "source": [
        "Recall, our dataset has different measures. Let's look at them again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDgsWWnX8PnP"
      },
      "source": [
        "daily_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "630QZ8Zo8TUv"
      },
      "source": [
        "**What is it that we want to predict?**\n",
        "\n",
        "*   Univariate Forecasting - choose one specific measure that you want to carry out predictions on. Only need one model.\n",
        "*   Multivariate Forecasting - carry out predictions on multiple measures. Build a model for each measure OR feed each as a separate channel to the neural network (remember how we treat RGB channels in computer vision).\n",
        "\n",
        "**First.** We will build a univariate prediction model to predict the next week consumption levels of total active power in the household (that is, column 0). Notice, predicting the next week consumption, day by day, is a multi-step ahead prediction where we are predicting SEVEN future values at once.\n",
        "\n",
        "If we were doing one day ahead prediction, this would correspond to one-step ahead prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW1uSlQ29PUc"
      },
      "source": [
        "# split the data for \"total active power\" -- (we had already done this previously)\n",
        "X_train, y_train = ---\n",
        "X_test, y_test = ---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxv5asfx9Vna"
      },
      "source": [
        "Let's build our prediction model!\n",
        "\n",
        "**What type of neural network would we use here?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em5NSd69-eCy"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pru3jntw95Wp"
      },
      "source": [
        "We will run a Long-Short Term Memory (LSTM) network since we are dealing with sequential data *(similar to dealing with language)*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXwmmopl8Lic"
      },
      "source": [
        "# let's define some parameters for our LSTM model\n",
        "n_inputs, n_channels, n_outputs = X_train.shape[1], X_train.shape[2], X_train.shape[1]\n",
        "n_cells, n_neurons = 64, 20\n",
        "\n",
        "# build the network archiecture\n",
        "---\n",
        "\n",
        "# compile the model\n",
        "# we define the mean squared error as an evaluation metric for training & define ADAM as an optimization algorithm\n",
        "---\n",
        "\n",
        "# train the network\n",
        "---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QloKlxFwZvy6"
      },
      "source": [
        "# **4. Evaluating Time-series Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzJ4SHFK_Nnd"
      },
      "source": [
        "Recall, we want our input (X) and output (y) to look like:\n",
        "\n",
        "```\n",
        "[Input], [Output]\n",
        "[d01, d02, d03, d04, d05, d06, d07], [d08, d09, d10, d11, d12, d13, d14]\n",
        "[d02, d03, d04, d05, d06, d07, d08], [d09, d10, d11, d12, d13, d14, d15]\n",
        "Etc ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2XkIi6t_2-O"
      },
      "source": [
        "#### Walk-forward validation setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh4cMS-q_tDe"
      },
      "source": [
        "# make a single prediction\n",
        "def forecast(model, history, n_input):\n",
        "\t# flatten data\n",
        "\tdata = np.array(history)\n",
        "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
        "\n",
        "  # retrieve last observations for input data\n",
        "\tinput_x = data[-n_input:, 0]\n",
        "\n",
        "  # reshape into [1, n_input, 1]\n",
        "\tinput_x = input_x.reshape((1, len(input_x), 1))\n",
        "\n",
        "  # forecast the next week\n",
        "\tpredicted_y = model.predict(input_x, verbose=0)\n",
        "\n",
        "  # we only want the vector forecast\n",
        "\tpredicted_y = predicted_y[0]\n",
        "\treturn predicted_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrcmpBSr_1Er"
      },
      "source": [
        "# To start evaluating our model, we can start from the last week in our training data\n",
        "history = [x for x in X_train] # converting history into a list\n",
        "n_input = 7\n",
        "\n",
        "# We do walk-forward validation for each week\n",
        "predictions = []\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "\t# predict the week\n",
        "\ty_predicted = forecast(model, history, n_input)\n",
        "\t# collect predictions\n",
        "\tpredictions.append(y_predicted)\n",
        "\t# get real observation and add it to my history\n",
        "\thistory.append(X_test[i,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKxiOwpr_6Pn"
      },
      "source": [
        "#### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm0ytngp_5aS"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Now, we want to evaluate our model\n",
        "predictions = np.array(predictions) # converting from list to np.array\n",
        "\n",
        "# But we want to see how well our model is doing day by day\n",
        "scores = []\n",
        "# calculate an RMSE score for each day\n",
        "for i in range(y_test.shape[1]): # Loop over the days of each week (shape[1] refers to the days)\n",
        "  # calculate mse for each day\n",
        "  mse = mean_squared_error(y_test[:, i], predictions[:, i])\n",
        "  rmse = np.sqrt(mse)\n",
        "\t# store\n",
        "  scores.append(rmse)\n",
        "\n",
        "# calculate overall RMSE for the entire week\n",
        "weekly_score = np.array(scores).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN5sdWrh_wsC"
      },
      "source": [
        "# print and plot scores\n",
        "days = ['mon', 'tue', 'wed', 'thr', 'fri', 'sat', 'sun']\n",
        "print('[AVG] %s: \\n [%.3f] %s' % (days, weekly_score, scores))\n",
        "\n",
        "plt.plot(days, scores, marker='o', label='lstm')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpFe1vgZE8P"
      },
      "source": [
        "### Visual validation of model performance: Predictions versus Observations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcRh2kLqZFQd"
      },
      "source": [
        "# storing observations and model outputs in dataframes\n",
        "true = pd.DataFrame(y_test[:,0]) # first day 2009-09-21\n",
        "pred = pd.DataFrame(predictions[:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoaYKWCRZGpt"
      },
      "source": [
        "# creating a datetime index for the dataframes\n",
        "dti = pd.date_range('2009-09-21', periods=427, freq='D') # first day of test samples: 2009-09-21"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5fH6S31ZIFR"
      },
      "source": [
        "# overlay observed sequence and predicted sequence\n",
        "ax = true.plot()\n",
        "pred.plot(ax=ax, alpha=.8, figsize=(14, 7))\n",
        "\n",
        "xi = list(range(len(dti)))\n",
        "plt.xticks(np.arange(min(xi), max(xi), 50), dti.date[np.arange(min(xi), max(xi), 50)])\n",
        "\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Global Active Power (kW/day)')\n",
        "plt.legend(['observed','predictions'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcz_uvYjdtOs"
      },
      "source": [
        "**References**\n",
        "\n",
        "The dataset was downloaded from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption#).\n",
        "\n",
        "Tutorial inspired by [Machine Learning Mastery](https://machinelearningmastery.com/multi-step-time-series-forecasting-with-machine-learning-models-for-household-electricity-consumption/)"
      ]
    }
  ]
}